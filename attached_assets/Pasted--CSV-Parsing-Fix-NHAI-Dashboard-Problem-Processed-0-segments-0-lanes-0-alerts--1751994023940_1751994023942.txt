# CSV Parsing Fix - NHAI Dashboard

## üö® **Problem: "Processed: 0 segments, 0 lanes, 0 alerts"**

This means your file upload is working, but the data parsing logic isn't recognizing the CSV structure.

## üîß **Quick Fix Solutions**

### **Solution 1: Check Your CSV Format**

Your CSV should have these columns in this order:
```
NH Number | Start Chainage | End Chainage | Length | Structure Details | [GPS Coordinates] | [Distress Data]
```

**Example of correct CSV format:**
```csv
NH148N,247310,247400,90,,26.36114,76.25048,26.36034,76.25034,...,1727,1853,1758,3001
NH148N,247400,247500,100,,26.36034,76.25034,26.35945,76.25018,...,1316,1270,1377,1462
```

### **Solution 2: Update the Parsing Logic**

**Replace your current parsing code with this:**

```javascript
// server/routes/segments.js - Updated parsing function
router.post('/upload', upload.single('file'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    let jsonData;
    
    // Handle both CSV and Excel files
    if (req.file.originalname.endsWith('.csv')) {
      // Parse CSV file
      const csvText = req.file.buffer.toString('utf8');
      const Papa = require('papaparse');
      const parsed = Papa.parse(csvText, {
        header: false,
        skipEmptyLines: true,
        delimiter: ',',
        dynamicTyping: true
      });
      jsonData = parsed.data;
    } else {
      // Parse Excel file
      const workbook = XLSX.read(req.file.buffer, { type: 'buffer' });
      const worksheet = workbook.Sheets[workbook.SheetNames[0]];
      jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 });
    }

    console.log('Raw data rows:', jsonData.length);
    console.log('Sample row:', jsonData[0]);

    // Skip header rows - adjust based on your file structure
    let dataStartRow = 0;
    
    // Find the first row with actual data (not headers)
    for (let i = 0; i < Math.min(jsonData.length, 10); i++) {
      const row = jsonData[i];
      if (row[0] && typeof row[0] === 'string' && row[0].includes('NH')) {
        dataStartRow = i;
        break;
      }
    }

    console.log('Data starts at row:', dataStartRow);
    const dataRows = jsonData.slice(dataStartRow);
    console.log('Processing rows:', dataRows.length);

    const processedSegments = [];
    const client = await pool.connect();

    try {
      await client.query('BEGIN');

      for (let i = 0; i < dataRows.length; i++) {
        const row = dataRows[i];
        
        // Skip empty rows
        if (!row || !row[0] || !row[1] || !row[2]) {
          console.log(`Skipping row ${i}: insufficient data`);
          continue;
        }

        console.log(`Processing row ${i}:`, row.slice(0, 5));

        // Extract basic segment info
        const highwayName = row[0];
        const startChainage = parseInt(row[1]);
        const endChainage = parseInt(row[2]);
        const length = parseInt(row[3]);
        const structureDetails = row[4] || null;

        if (!highwayName || !startChainage || !endChainage) {
          console.log(`Skipping row ${i}: invalid basic data`);
          continue;
        }

        // Insert or get highway
        let highwayResult = await client.query(
          'SELECT id FROM highways WHERE name = $1',
          [highwayName]
        );

        if (highwayResult.rows.length === 0) {
          highwayResult = await client.query(
            'INSERT INTO highways (name, state) VALUES ($1, $2) RETURNING id',
            [highwayName, 'Unknown']
          );
        }

        const highwayId = highwayResult.rows[0].id;

        // Insert segment
        const segmentResult = await client.query(`
          INSERT INTO nsv_segments (highway_id, start_chainage, end_chainage, length, structure_details)
          VALUES ($1, $2, $3, $4, $5)
          RETURNING id
        `, [highwayId, startChainage, endChainage, length, structureDetails]);

        const segmentId = segmentResult.rows[0].id;

        // Process lane data - adjust indices based on your CSV structure
        const lanes = ['L1', 'L2', 'L3', 'L4', 'R1', 'R2', 'R3', 'R4'];
        let lanesProcessed = 0;

        for (let laneIndex = 0; laneIndex < lanes.length; laneIndex++) {
          const laneId = lanes[laneIndex];
          
          // Adjust these indices based on your CSV column structure
          const coordStartIndex = 5 + (laneIndex * 4); // GPS coordinates
          const roughnessIndex = 37 + laneIndex; // Roughness BI values
          const rutDepthIndex = 46 + laneIndex; // Rut depth values
          const crackAreaIndex = 55 + laneIndex; // Crack area values
          const ravellingIndex = 64 + laneIndex; // Ravelling values

          // Extract data with null checks
          const startLat = row[coordStartIndex] || null;
          const startLng = row[coordStartIndex + 1] || null;
          const endLat = row[coordStartIndex + 2] || null;
          const endLng = row[coordStartIndex + 3] || null;
          const roughnessBi = row[roughnessIndex] || null;
          const rutDepth = row[rutDepthIndex] || null;
          const crackArea = row[crackAreaIndex] || null;
          const ravelling = row[ravellingIndex] || null;

          // Insert lane data
          await client.query(`
            INSERT INTO lane_data (
              segment_id, lane_id, start_lat, start_lng, end_lat, end_lng,
              roughness_bi, rut_depth, crack_area, ravelling
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
          `, [
            segmentId, laneId, startLat, startLng, endLat, endLng,
            roughnessBi, rutDepth, crackArea, ravelling
          ]);

          lanesProcessed++;
        }

        console.log(`Processed segment ${segmentId} with ${lanesProcessed} lanes`);
        processedSegments.push(segmentId);
      }

      await client.query('COMMIT');

      // Generate alerts after all data is inserted
      const alertsGenerated = await generateAlerts(client);

      res.json({
        success: true,
        message: 'File processed successfully',
        segments: processedSegments.length,
        lanes: processedSegments.length * 8,
        alerts: alertsGenerated,
        details: {
          totalRows: dataRows.length,
          processedSegments: processedSegments.length,
          skippedRows: dataRows.length - processedSegments.length
        }
      });

    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }

  } catch (error) {
    console.error('Error processing file:', error);
    res.status(500).json({ 
      error: 'File processing failed',
      details: error.message,
      stack: process.env.NODE_ENV === 'development' ? error.stack : undefined
    });
  }
});

// Helper function to generate alerts
async function generateAlerts(client) {
  const thresholds = {
    roughness_bi: 2400,
    rut_depth: 5,
    crack_area: 5,
    ravelling: 1
  };

  let alertsGenerated = 0;

  for (const [metric, threshold] of Object.entries(thresholds)) {
    const result = await client.query(`
      INSERT INTO alerts (segment_id, lane_id, alert_type, current_value, threshold_value, severity)
      SELECT 
        ld.segment_id,
        ld.lane_id,
        $1 as alert_type,
        ld.${metric} as current_value,
        $2 as threshold_value,
        CASE 
          WHEN ld.${metric} > $2 THEN 'critical'
          WHEN ld.${metric} > $2 * 0.8 THEN 'high'
          WHEN ld.${metric} > $2 * 0.6 THEN 'medium'
          ELSE 'low'
        END as severity
      FROM lane_data ld
      WHERE ld.${metric} IS NOT NULL 
        AND ld.${metric} > $2 * 0.6
      RETURNING id
    `, [metric, threshold]);

    alertsGenerated += result.rows.length;
  }

  return alertsGenerated;
}
```

### **Solution 3: Debug Your CSV Structure**

**Add this debug endpoint to check your file structure:**

```javascript
// Add this route to debug your CSV
router.post('/debug-upload', upload.single('file'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    let jsonData;
    
    if (req.file.originalname.endsWith('.csv')) {
      const csvText = req.file.buffer.toString('utf8');
      const Papa = require('papaparse');
      const parsed = Papa.parse(csvText, {
        header: false,
        skipEmptyLines: true,
        delimiter: ',',
        dynamicTyping: true
      });
      jsonData = parsed.data;
    } else {
      const workbook = XLSX.read(req.file.buffer, { type: 'buffer' });
      const worksheet = workbook.Sheets[workbook.SheetNames[0]];
      jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 });
    }

    res.json({
      fileName: req.file.originalname,
      totalRows: jsonData.length,
      firstFiveRows: jsonData.slice(0, 5),
      columnCount: jsonData[0]?.length || 0,
      sampleDataRow: jsonData.find(row => row[0] && row[0].toString().includes('NH'))
    });

  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
```

### **Solution 4: Test with Sample Data**

**Create a test CSV file with this format:**
```csv
NH148N,247310,247400,90,,26.36114,76.25048,26.36034,76.25034,26.36111,76.25052,26.36031,76.25038,26.36114,76.25056,26.36034,76.25042,26.36115,76.25061,26.36035,76.25047,26.36011,76.25005,26.36091,76.25019,26.35999,76.24998,26.36079,76.25013,26.35978,76.24992,26.36058,76.25006,26.36007,76.24993,26.36088,76.25007,2400,1727,1853,1758,3001,1285,2736,1877,2579,5,2.9,3.8,3.1,4.8,4.1,3.2,3.3,3.2,5,0.048,0.004,0.006,0.004,0,0,0,0,1,0,0.011,0.008,0,0.013,0,0,0
NH148N,247400,247500,100,,26.36034,76.25034,26.35945,76.25018,26.36031,76.25038,26.35942,76.25022,26.36034,76.25042,26.35945,76.25026,26.36035,76.25047,26.35946,76.25031,26.35922,76.24989,26.36011,76.25005,26.3591,76.24982,26.35999,76.24998,26.35889,76.24975,26.35978,76.24992,26.35918,76.24976,26.36007,76.24993,2400,1316,1270,1377,1462,1578,2918,1766,3591,5,3.3,4.6,3.3,4.1,3.7,3,3.2,3.6,5,0.008,0.003,0,0,0.001,0,0,0,1,0,0,0.003,0.113,0.013,0,0,0
```

## üîç **Quick Debugging Steps:**

1. **Check the debug endpoint** first to see your file structure
2. **Verify column mapping** matches your CSV format
3. **Test with sample data** to confirm parsing works
4. **Check database logs** for any constraint violations
5. **Add more console.log statements** to trace the parsing process

## üéØ **Expected Result After Fix:**

```
File uploaded successfully
Processed: 450 segments, 3,600 lanes, 127 alerts
```

This should resolve your parsing issue and populate your dashboard with actual data!